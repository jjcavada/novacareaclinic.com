---
phase: 04-technical-seo
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/app/sitemap.ts, src/app/robots.ts]
autonomous: true
---

<objective>
Implement technical SEO requirements for crawlability: XML sitemap and robots.txt configuration.

Purpose: Enable search engines to efficiently crawl and index all public pages while respecting crawl directives.
Output: Working sitemap.xml and robots.txt accessible at standard URLs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase context
@.planning/phases/03-rich-seo/03-01-SUMMARY.md

# Existing files
@src/app/layout.tsx
@next.config.mjs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create XML sitemap</name>
  <files>src/app/sitemap.ts</files>
  <action>
Create Next.js 14 sitemap.ts using the MetadataRoute.Sitemap type.

Include all 10 public pages:
- /homepage (priority 1.0, daily changefreq)
- /about (priority 0.8, monthly)
- /services (priority 0.9, monthly)
- /providers (priority 0.8, monthly)
- /patient-resources (priority 0.7, weekly - resources updated periodically)
- /patient-portal (priority 0.6, monthly)
- /contact (priority 0.8, monthly)
- /insurance-and-payment (priority 0.7, monthly)
- /schedule-appointment (priority 0.8, monthly)
- /legal-compliance-hub (priority 0.3, yearly)

Use base URL: https://novacareclinic.com (matches metadataBase in layout.tsx)
Set lastModified to current date for all pages.

Do NOT include root "/" (redirects to /homepage) in sitemap.
  </action>
  <verify>npm run build succeeds; curl http://localhost:4028/sitemap.xml returns valid XML with all URLs</verify>
  <done>sitemap.xml accessible at /sitemap.xml with all 10 pages listed with correct priorities</done>
</task>

<task type="auto">
  <name>Task 2: Create robots.txt</name>
  <files>src/app/robots.ts</files>
  <action>
Create Next.js 14 robots.ts using MetadataRoute.Robots type.

Configuration:
- User-Agent: * (all crawlers)
- Allow: / (allow all paths)
- Disallow: none (all pages are public)
- Sitemap: https://novacareclinic.com/sitemap.xml

Do NOT disallow any paths - this is a public marketing site with no private sections.
The patient-portal page is informational only (no actual login functionality on static site).
  </action>
  <verify>npm run build succeeds; curl http://localhost:4028/robots.txt returns valid robots.txt with sitemap reference</verify>
  <done>robots.txt accessible at /robots.txt with Allow: / and sitemap reference</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without errors
- [ ] /sitemap.xml returns valid XML with 10 URLs
- [ ] /robots.txt returns valid content with sitemap reference
- [ ] All URLs in sitemap use https://novacareclinic.com base
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Sitemap includes all 10 public pages with appropriate priorities
- robots.txt allows all crawlers with sitemap directive
</success_criteria>

<output>
After completion, create `.planning/phases/04-technical-seo/04-01-SUMMARY.md`
</output>
